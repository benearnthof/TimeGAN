{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1c59f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data_loading import sine_data_generation\n",
    "from utils import random_generator\n",
    "from data_loading import MinMaxScaler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from utils import extract_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e849290",
   "metadata": {},
   "source": [
    "Define Class for Module Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294d9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_GAN_module(nn.Module):\n",
    "    \"\"\"\n",
    "    Class from which a module of the Time GAN Architecture can be constructed, \n",
    "    consisting of a n_layer stacked RNN layers and a fully connected layer\n",
    "    \n",
    "    input_size = dim of data (depending if module operates on latent or non-latent space)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, activation=torch.sigmoid):\n",
    "        super(Time_GAN_module, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.sigma = activation\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            # Initializing hidden state for first input using method defined below\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "\n",
    "            # Passing in the input and hidden state into the model and obtaining outputs\n",
    "            out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "            # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "            out = out.contiguous().view(-1, self.hidden_dim)\n",
    "            out = self.fc(out)\n",
    "            \n",
    "            if self.sigma == nn.Identity:\n",
    "                idendity = nn.Identity()\n",
    "                return idendity(out)\n",
    "                \n",
    "            out = self.sigma(out)\n",
    "            \n",
    "            # HIDDEN STATES WERDEN IN DER PAPER IMPLEMENTIERUNG AUCH COMPUTED, ALLERDINGS NICHT BENUTZT?\n",
    "            \n",
    "            return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad1d5e",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2e3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "output_size = 20\n",
    "hidden_dim = 20\n",
    "n_layers = 3\n",
    "gamma = 1\n",
    "\n",
    "no, seq_len, dim = 12800, 24, 5 \n",
    "\n",
    "batch_size = 128\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b74eb",
   "metadata": {},
   "source": [
    "Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2373fb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12800, 24, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sine_data_generation(no, seq_len, dim)\n",
    "data = MinMaxScaler(data)\n",
    "data = torch.Tensor(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9892c",
   "metadata": {},
   "source": [
    "Create Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db4e7203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time_GAN_module(\n",
       "  (rnn): GRU(5, 20, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedder = Time_GAN_module(input_size=dim, output_size=hidden_dim, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9edcdcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time_GAN_module(\n",
       "  (rnn): GRU(20, 20, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Recovery = Time_GAN_module(input_size=hidden_dim, output_size=dim, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "700d9516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time_GAN_module(\n",
       "  (rnn): GRU(5, 20, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Generator = Time_GAN_module(input_size=dim, output_size=hidden_dim, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "771283fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time_GAN_module(\n",
       "  (rnn): GRU(20, 20, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Supervisor = Time_GAN_module(input_size=hidden_dim, output_size=hidden_dim, hidden_dim=hidden_dim, n_layers=n_layers-1)\n",
    "Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03e296b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time_GAN_module(\n",
       "  (rnn): GRU(20, 20, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discriminator = Time_GAN_module(input_size=hidden_dim, output_size=1, hidden_dim=hidden_dim, n_layers=n_layers, \n",
    "                               activation=nn.Identity)\n",
    "Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f997b",
   "metadata": {},
   "source": [
    "Create Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7aed175",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_optimizer = optim.Adam(Embedder.parameters(), lr=0.001)\n",
    "recovery_optimizer = optim.Adam(Recovery.parameters(), lr=0.001)\n",
    "supervisor_optimizer = optim.Adam(Recovery.parameters(), lr=0.001)\n",
    "discriminator_optimizer = optim.Adam(Discriminator.parameters(), lr=0.001)\n",
    "generator_optimizer = optim.Adam(Generator.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05ad143",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f04458be",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd5039",
   "metadata": {},
   "source": [
    "Embedder Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd9274ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Embedding Network Training\n",
      "step: 1/100, e_loss: 1.2113627\n",
      "step: 2/100, e_loss: 1.1118678\n",
      "step: 3/100, e_loss: 1.0656068\n",
      "step: 4/100, e_loss: 1.0669308\n",
      "step: 5/100, e_loss: 0.99132323\n",
      "step: 6/100, e_loss: 1.0290759\n",
      "step: 7/100, e_loss: 1.05712\n",
      "step: 8/100, e_loss: 0.9705321\n",
      "step: 9/100, e_loss: 0.95293725\n",
      "step: 10/100, e_loss: 0.917935\n",
      "step: 11/100, e_loss: 0.7142837\n",
      "step: 12/100, e_loss: 0.632312\n",
      "step: 13/100, e_loss: 0.57735693\n",
      "step: 14/100, e_loss: 0.56844777\n",
      "step: 15/100, e_loss: 0.5494405\n",
      "step: 16/100, e_loss: 0.51849407\n",
      "step: 17/100, e_loss: 0.5033815\n",
      "step: 18/100, e_loss: 0.48689562\n",
      "step: 19/100, e_loss: 0.49090096\n",
      "step: 20/100, e_loss: 0.46919236\n",
      "step: 21/100, e_loss: 0.4618927\n",
      "step: 22/100, e_loss: 0.45426735\n",
      "step: 23/100, e_loss: 0.45190057\n",
      "step: 24/100, e_loss: 0.44268012\n",
      "step: 25/100, e_loss: 0.42174116\n",
      "step: 26/100, e_loss: 0.40710518\n",
      "step: 27/100, e_loss: 0.41366136\n",
      "step: 28/100, e_loss: 0.40219304\n",
      "step: 29/100, e_loss: 0.39501354\n",
      "step: 30/100, e_loss: 0.3933027\n",
      "step: 31/100, e_loss: 0.39565063\n",
      "step: 32/100, e_loss: 0.38209844\n",
      "step: 33/100, e_loss: 0.39158872\n",
      "step: 34/100, e_loss: 0.37187976\n",
      "step: 35/100, e_loss: 0.37577257\n",
      "step: 36/100, e_loss: 0.3895635\n",
      "step: 37/100, e_loss: 0.37357053\n",
      "step: 38/100, e_loss: 0.37570268\n",
      "step: 39/100, e_loss: 0.3709172\n",
      "step: 40/100, e_loss: 0.3795162\n",
      "step: 41/100, e_loss: 0.37352335\n",
      "step: 42/100, e_loss: 0.36841476\n",
      "step: 43/100, e_loss: 0.36680374\n",
      "step: 44/100, e_loss: 0.3564397\n",
      "step: 45/100, e_loss: 0.35802832\n",
      "step: 46/100, e_loss: 0.35769823\n",
      "step: 47/100, e_loss: 0.37051776\n",
      "step: 48/100, e_loss: 0.34998485\n",
      "step: 49/100, e_loss: 0.35414663\n",
      "step: 50/100, e_loss: 0.35055628\n",
      "step: 51/100, e_loss: 0.34677535\n",
      "step: 52/100, e_loss: 0.3563878\n",
      "step: 53/100, e_loss: 0.35192564\n",
      "step: 54/100, e_loss: 0.34952092\n",
      "step: 55/100, e_loss: 0.3450629\n",
      "step: 56/100, e_loss: 0.34822628\n",
      "step: 57/100, e_loss: 0.3484381\n",
      "step: 58/100, e_loss: 0.34875724\n",
      "step: 59/100, e_loss: 0.34803674\n",
      "step: 60/100, e_loss: 0.34742665\n",
      "step: 61/100, e_loss: 0.3473913\n",
      "step: 62/100, e_loss: 0.3325091\n",
      "step: 63/100, e_loss: 0.34766918\n",
      "step: 64/100, e_loss: 0.34503502\n",
      "step: 65/100, e_loss: 0.33560267\n",
      "step: 66/100, e_loss: 0.33467916\n",
      "step: 67/100, e_loss: 0.34391943\n",
      "step: 68/100, e_loss: 0.33348498\n",
      "step: 69/100, e_loss: 0.3313439\n",
      "step: 70/100, e_loss: 0.34674338\n",
      "step: 71/100, e_loss: 0.33818918\n",
      "step: 72/100, e_loss: 0.3278406\n",
      "step: 73/100, e_loss: 0.32323113\n",
      "step: 74/100, e_loss: 0.33657125\n",
      "step: 75/100, e_loss: 0.33028233\n",
      "step: 76/100, e_loss: 0.3220859\n",
      "step: 77/100, e_loss: 0.33022386\n",
      "step: 78/100, e_loss: 0.33329108\n",
      "step: 79/100, e_loss: 0.3254626\n",
      "step: 80/100, e_loss: 0.3294356\n",
      "step: 81/100, e_loss: 0.32252732\n",
      "step: 82/100, e_loss: 0.32765317\n",
      "step: 83/100, e_loss: 0.33407626\n",
      "step: 84/100, e_loss: 0.32759583\n",
      "step: 85/100, e_loss: 0.3227875\n",
      "step: 86/100, e_loss: 0.3191188\n",
      "step: 87/100, e_loss: 0.31953356\n",
      "step: 88/100, e_loss: 0.3107127\n",
      "step: 89/100, e_loss: 0.3244745\n",
      "step: 90/100, e_loss: 0.31598473\n",
      "step: 91/100, e_loss: 0.31910077\n",
      "step: 92/100, e_loss: 0.31451318\n",
      "step: 93/100, e_loss: 0.3298\n",
      "step: 94/100, e_loss: 0.30654082\n",
      "step: 95/100, e_loss: 0.315102\n",
      "step: 96/100, e_loss: 0.31318155\n",
      "step: 97/100, e_loss: 0.32194522\n",
      "step: 98/100, e_loss: 0.30974567\n",
      "step: 99/100, e_loss: 0.31215578\n",
      "Finish Embedding Network Training\n"
     ]
    }
   ],
   "source": [
    "print('Start Embedding Network Training')\n",
    "\n",
    "for e in range(epoch): \n",
    "    for batch_index, X in enumerate(loader):\n",
    "        \n",
    "        MSE_loss = nn.MSELoss()\n",
    "        \n",
    "        H, _ = Embedder(X.float())\n",
    "        H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "        X_tilde, _ = Recovery(H)\n",
    "        X_tilde = torch.reshape(X_tilde, (batch_size, seq_len, dim))\n",
    "\n",
    "        E_loss0 = 10 * torch.sqrt(MSE_loss(X, X_tilde))  \n",
    "\n",
    "        Embedder.zero_grad()\n",
    "        Recovery.zero_grad()\n",
    "\n",
    "        E_loss0.backward(retain_graph=True)\n",
    "\n",
    "        embedder_optimizer.step()\n",
    "        recovery_optimizer.step()\n",
    "\n",
    "        if e in range(1,epoch) and batch_index == 0:\n",
    "            print('step: '+ str(e) + '/' + str(epoch) + ', e_loss: ' + str(np.sqrt(E_loss0.detach().numpy())))\n",
    "\n",
    "print('Finish Embedding Network Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1634b07",
   "metadata": {},
   "source": [
    "Training with supervised Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c75a988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training with Supervised Loss Only\n",
      "step: 1/100, s_loss: 0.09964683\n",
      "step: 2/100, s_loss: 0.09478158\n",
      "step: 3/100, s_loss: 0.092493\n",
      "step: 4/100, s_loss: 0.08582403\n",
      "step: 5/100, s_loss: 0.083220385\n",
      "step: 6/100, s_loss: 0.07979374\n",
      "step: 7/100, s_loss: 0.074106425\n",
      "step: 8/100, s_loss: 0.07030248\n",
      "step: 9/100, s_loss: 0.072493605\n",
      "step: 10/100, s_loss: 0.06692988\n",
      "step: 11/100, s_loss: 0.06455012\n",
      "step: 12/100, s_loss: 0.06289997\n",
      "step: 13/100, s_loss: 0.059171397\n",
      "step: 14/100, s_loss: 0.0561611\n",
      "step: 15/100, s_loss: 0.053076327\n",
      "step: 16/100, s_loss: 0.05145452\n",
      "step: 17/100, s_loss: 0.047989562\n",
      "step: 18/100, s_loss: 0.044613775\n",
      "step: 19/100, s_loss: 0.042740956\n",
      "step: 20/100, s_loss: 0.03997947\n",
      "step: 21/100, s_loss: 0.038406666\n",
      "step: 22/100, s_loss: 0.038452793\n",
      "step: 23/100, s_loss: 0.035605773\n",
      "step: 24/100, s_loss: 0.03442791\n",
      "step: 25/100, s_loss: 0.032908514\n",
      "step: 26/100, s_loss: 0.03171904\n",
      "step: 27/100, s_loss: 0.029951302\n",
      "step: 28/100, s_loss: 0.028834099\n",
      "step: 29/100, s_loss: 0.028288601\n",
      "step: 30/100, s_loss: 0.026867647\n",
      "step: 31/100, s_loss: 0.024702491\n",
      "step: 32/100, s_loss: 0.02528292\n",
      "step: 33/100, s_loss: 0.023304423\n",
      "step: 34/100, s_loss: 0.022734763\n",
      "step: 35/100, s_loss: 0.022860697\n",
      "step: 36/100, s_loss: 0.021795161\n",
      "step: 37/100, s_loss: 0.020720478\n",
      "step: 38/100, s_loss: 0.02108492\n",
      "step: 39/100, s_loss: 0.0195224\n",
      "step: 40/100, s_loss: 0.018501285\n",
      "step: 41/100, s_loss: 0.01850534\n",
      "step: 42/100, s_loss: 0.01730817\n",
      "step: 43/100, s_loss: 0.017487042\n",
      "step: 44/100, s_loss: 0.016650064\n",
      "step: 45/100, s_loss: 0.01586118\n",
      "step: 46/100, s_loss: 0.015419619\n",
      "step: 47/100, s_loss: 0.01509535\n",
      "step: 48/100, s_loss: 0.014559614\n",
      "step: 49/100, s_loss: 0.014284575\n",
      "step: 50/100, s_loss: 0.013610347\n",
      "step: 51/100, s_loss: 0.013406681\n",
      "step: 52/100, s_loss: 0.013053508\n",
      "step: 53/100, s_loss: 0.0126679875\n",
      "step: 54/100, s_loss: 0.012118604\n",
      "step: 55/100, s_loss: 0.011981901\n",
      "step: 56/100, s_loss: 0.011179211\n",
      "step: 57/100, s_loss: 0.011012429\n",
      "step: 58/100, s_loss: 0.010738596\n",
      "step: 59/100, s_loss: 0.010255175\n",
      "step: 60/100, s_loss: 0.00967701\n",
      "step: 61/100, s_loss: 0.009615674\n",
      "step: 62/100, s_loss: 0.009379623\n",
      "step: 63/100, s_loss: 0.009103575\n",
      "step: 64/100, s_loss: 0.008910764\n",
      "step: 65/100, s_loss: 0.008584712\n",
      "step: 66/100, s_loss: 0.008442634\n",
      "step: 67/100, s_loss: 0.007933528\n",
      "step: 68/100, s_loss: 0.007787604\n",
      "step: 69/100, s_loss: 0.0076222993\n",
      "step: 70/100, s_loss: 0.007446125\n",
      "step: 71/100, s_loss: 0.0069868346\n",
      "step: 72/100, s_loss: 0.0070617944\n",
      "step: 73/100, s_loss: 0.0068392023\n",
      "step: 74/100, s_loss: 0.006529394\n",
      "step: 75/100, s_loss: 0.006536999\n",
      "step: 76/100, s_loss: 0.0063466104\n",
      "step: 77/100, s_loss: 0.0062261326\n",
      "step: 78/100, s_loss: 0.0061889603\n",
      "step: 79/100, s_loss: 0.0059173787\n",
      "step: 80/100, s_loss: 0.0057402933\n",
      "step: 81/100, s_loss: 0.005795222\n",
      "step: 82/100, s_loss: 0.0055537564\n",
      "step: 83/100, s_loss: 0.0053505716\n",
      "step: 84/100, s_loss: 0.0050878227\n",
      "step: 85/100, s_loss: 0.0049465117\n",
      "step: 86/100, s_loss: 0.004773664\n",
      "step: 87/100, s_loss: 0.0046948455\n",
      "step: 88/100, s_loss: 0.0048179557\n",
      "step: 89/100, s_loss: 0.0045903055\n",
      "step: 90/100, s_loss: 0.004341908\n",
      "step: 91/100, s_loss: 0.00444895\n",
      "step: 92/100, s_loss: 0.0041767126\n",
      "step: 93/100, s_loss: 0.0041216942\n",
      "step: 94/100, s_loss: 0.003941076\n",
      "step: 95/100, s_loss: 0.0038688513\n",
      "step: 96/100, s_loss: 0.0037284908\n",
      "step: 97/100, s_loss: 0.003678462\n",
      "step: 98/100, s_loss: 0.0034989333\n",
      "step: 99/100, s_loss: 0.0034186372\n",
      "Finish Training with Supervised Loss Only\n"
     ]
    }
   ],
   "source": [
    "print('Start Training with Supervised Loss Only')\n",
    "\n",
    "for e in range(epoch): \n",
    "    for batch_index, X in enumerate(loader):\n",
    "\n",
    "        H, _ = Embedder(X.float())\n",
    "        H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "        H_hat_supervise, _ = Supervisor(H)\n",
    "        H_hat_supervise = torch.reshape(H_hat_supervise, (batch_size, seq_len, hidden_dim))  \n",
    "\n",
    "        G_loss_S = MSE_loss(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "\n",
    "\n",
    "        Embedder.zero_grad()\n",
    "        Supervisor.zero_grad()\n",
    "\n",
    "        G_loss_S.backward(retain_graph=True)\n",
    "\n",
    "        embedder_optimizer.step()\n",
    "        supervisor_optimizer.step()\n",
    "\n",
    "        if e in range(1,epoch) and batch_index == 0:\n",
    "            print('step: '+ str(e) + '/' + str(epoch) + ', s_loss: ' + str(np.sqrt(G_loss_S.detach().numpy())))\n",
    "\n",
    "print('Finish Training with Supervised Loss Only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1040c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e2cd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = random_generator(batch_size=batch_size, z_dim=dim, \n",
    "                                       T_mb=extract_time(data)[0], max_seq_len=extract_time(data)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "627dd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data, batch_size, shuffle=True)\n",
    "\n",
    "random_loader = DataLoader(random_data, batch_size, shuffle=True)\n",
    "\n",
    "binary_cross_entropy_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "MSE_loss = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bede8319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Joint Training\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.18187945e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0969117e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.147849e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1307436e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1593437e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0761217e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0991596e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1246363e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1470942e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1571507e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0728089e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0862017e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0789463e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.16448e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1714253e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1338025e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0736785e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1565403e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1649264e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0923707e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0367358e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1037751e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0768112e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.0527578e-05, E_loss_t0: nan\n",
      "step: 0/2, G_loss_U: nan, G_loss_S: 1.1376204e-05, E_loss_t0: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-dc5747c11203>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mG_loss_U\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mG_loss_V\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Start Joint Training')\n",
    "\n",
    "for e in range(epoch): \n",
    "\n",
    "    for batch_index, X in enumerate(loader):\n",
    "        \n",
    "        random_data = random_generator(batch_size=batch_size, z_dim=dim, \n",
    "                                       T_mb=extract_time(data)[0], max_seq_len=extract_time(data)[1])\n",
    "        \n",
    "        \n",
    "        # Generator Training \n",
    "        ## Train Generator\n",
    "        z = torch.tensor(random_data)\n",
    "        z = z.float()\n",
    "        \n",
    "        e_hat, _ = Generator(z)\n",
    "        e_hat = torch.reshape(e_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "        H_hat, _ = Supervisor(e_hat)\n",
    "        H_hat = torch.reshape(H_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "        Y_fake = Discriminator(H_hat)\n",
    "        Y_fake = torch.reshape(Y_fake, (batch_size, seq_len, 1))\n",
    "        \n",
    "        x_hat, _ = Recovery(H_hat)\n",
    "        x_hat = torch.reshape(x_hat, (batch_size, seq_len, dim))\n",
    "        \n",
    "        \n",
    "        Generator.zero_grad()\n",
    "        Supervisor.zero_grad()\n",
    "        Discriminator.zero_grad()\n",
    "        Recovery.zero_grad()\n",
    "        \n",
    "        G_loss_U = binary_cross_entropy_loss(torch.ones_like(Y_fake), Y_fake)\n",
    "        \n",
    "        \n",
    "        \n",
    "        G_loss_V1 = torch.mean(torch.abs((torch.std(x_hat, [0], unbiased = False)) + 1e-6 - (torch.std(X, [0]) + 1e-6)))\n",
    "        G_loss_V2 = torch.mean(torch.abs((torch.mean(x_hat, [0]) - (torch.mean(X, [0])))))\n",
    "        G_loss_V = G_loss_V1 + G_loss_V2\n",
    "        \n",
    " \n",
    "        G_loss_U.backward(retain_graph=True)\n",
    "        G_loss_V.backward()\n",
    "\n",
    "\n",
    "        generator_optimizer.step()\n",
    "        supervisor_optimizer.step()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        ## Train Embedder\n",
    "        \n",
    "        MSE_loss = nn.MSELoss()\n",
    "        \n",
    "        H, _ = Embedder(X.float())\n",
    "        H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "        X_tilde, _ = Recovery(H)\n",
    "        X_tilde = torch.reshape(X_tilde, (batch_size, seq_len, dim))\n",
    "\n",
    "        E_loss0 = 10 * torch.sqrt(MSE_loss(X, X_tilde))  \n",
    "        \n",
    "        H_hat_supervise, _ = Supervisor(H)\n",
    "        H_hat_supervise = torch.reshape(H_hat_supervise, (batch_size, seq_len, hidden_dim))  \n",
    "\n",
    "        G_loss_S = MSE_loss(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "        E_loss = E_loss0  + 0.1 * G_loss_S\n",
    "        \n",
    "        G_loss_S.backward(retain_graph=True)\n",
    "        E_loss.backward()\n",
    "        \n",
    "        Embedder.zero_grad()\n",
    "        Recovery.zero_grad()\n",
    "        Supervisor.zero_grad()\n",
    "        \n",
    "        embedder_optimizer.step()\n",
    "        recovery_optimizer.step()\n",
    "        supervisor_optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Train Discriminator \n",
    "        #....\n",
    "        \n",
    "        \n",
    "        \n",
    "        #if e in range(1,epoch) and batch_index == 0:\n",
    "        print('step: '+ str(e) + '/' + str(epoch) + ', G_loss_U: ' + str(G_loss_U.detach().numpy()) + ', G_loss_S: ' + \n",
    "             str(G_loss_S.detach().numpy()) + ', E_loss_t0: ' + str(np.sqrt(E_loss0.detach().numpy()))\n",
    "             )\n",
    "        \n",
    "        \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "print('Finish Joint Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
